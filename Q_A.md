# Your Knowledge Base for GeneRativeAi

**Explain Mixture of Expert Architecture**
- Mixture of Experts (MoE) is a neural network architecture designed to scale model capacity (the number of parameters) without a proportional increase in the computational cost (FLOPs).
- 
